{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Ingestia","text":"<p>Ingestia is a metadata-driven framework designed to transform documentation, architectural standards, and governance from static artifacts into executable parameters.</p> <p>Instead of treating documentation as the final step of a project, Ingestia makes it the starting point of execution.</p> <p>Architecture rules, naming standards, and integrity definitions are not described \u2014 they are enforced.</p>"},{"location":"#why-ingestia-exists","title":"Why Ingestia Exists","text":"<p>Modern Lakehouse projects operate under constant pressure:</p> <ul> <li>faster delivery cycles</li> <li>increasing data sources</li> <li>multiple domains working in parallel</li> <li>high expectations for data quality and traceability</li> </ul> <p>In this environment, documentation and governance often become bottlenecks.</p> <p>When over-engineered, they slow delivery. When ignored, they create chaos.</p> <p>Ingestia was created to resolve this tension.</p> <p>It ensures that:</p> <ul> <li>standards do not slow down execution</li> <li>governance is not optional</li> <li>ingestion is consistent across teams</li> <li>quality rules are executable, not theoretical</li> </ul>"},{"location":"#the-core-principle","title":"The Core Principle","text":"<p>Documentation is not an artifact. It is a parameter.</p> <p>Ingestia is built around a metadata-driven approach where:</p> <ul> <li>structural definitions</li> <li>naming conventions</li> <li>integrity rules</li> <li>execution behavior</li> </ul> <p>are defined in dictionaries and consumed directly by the ingestion engine.</p> <p>This eliminates hidden logic inside notebooks and pipelines.</p> <p>Execution becomes deterministic, reproducible, and transparent.</p>"},{"location":"#what-ingestia-provides","title":"What Ingestia Provides","text":"<p>Ingestia defines a structured Lakehouse model composed of:</p> <ul> <li>Layering strategy (Raw, Standardized, Conformed, Serving)</li> <li>Metadata dictionaries that describe structure and constraints</li> <li>Execution engine that applies those definitions consistently</li> <li>Control columns and batching standards</li> <li>Integrity enforcement and quarantine strategy</li> </ul> <p>The result is a system where:</p> <ul> <li>projects remain standardized</li> <li>delivery speed is maintained</li> <li>governance scales with growth</li> <li>environments remain manageable over time</li> </ul>"},{"location":"#who-it-is-for","title":"Who It Is For","text":"<p>Ingestia is designed for:</p> <ul> <li>Data Architects defining cross-domain standards</li> <li>Data Engineers building ingestion and transformation pipelines</li> <li>Platform teams implementing scalable LakeOps practices</li> </ul> <p>It is not a no-code ingestion tool. It is not a data warehouse methodology.</p> <p>It is an execution framework for disciplined Lakehouse architecture.</p>"},{"location":"#what-you-will-find-in-this-documentation","title":"What You Will Find in This Documentation","text":"<p>This documentation describes:</p> <ul> <li>The philosophy and design principles behind Ingestia</li> <li>The architectural model and layering strategy</li> <li>The metadata-driven structure that powers execution</li> <li>The ingestion engine and enforcement mechanisms</li> <li>The governance and quality model</li> <li>Patterns and anti-patterns observed in real-world projects</li> </ul> <p>If your goal is to deliver fast without sacrificing standards, Ingestia was built for that purpose.</p>"},{"location":"appendix/glossary/","title":"Gloassary","text":"<p>Section under development</p> <p>This module is fully designed. Detailed documentation is being progressively published.</p>"},{"location":"execution/batching/","title":"Batching","text":"<p>Section under development</p> <p>This module is fully designed. Detailed documentation is being progressively published.</p>"},{"location":"execution/constraints/","title":"Constraints","text":"<p>Section under development</p> <p>This module is fully designed. Detailed documentation is being progressively published.</p>"},{"location":"execution/data-quality/","title":"Data Quality","text":"<p>Section under development</p> <p>This module is fully designed. Detailed documentation is being progressively published.</p>"},{"location":"execution/ingest-engine/","title":"Ingest Engine","text":"<p>The Ingest Engine is the execution core of the Ingestia framework.</p> <p>It translates declarative metadata into deterministic, controlled, and reproducible data operations.</p> <p>Ingestia is not a collection of notebooks \u2014 it is a metadata-driven execution engine.</p>"},{"location":"execution/ingest-engine/#design-principles","title":"Design Principles","text":"<p>The Ingest Engine is built on the following principles:</p> <ul> <li>Metadata over hardcode  </li> <li>Deterministic execution  </li> <li>Idempotent by design  </li> <li>Explicit operational metadata  </li> <li>Clear separation between business and technical columns  </li> <li>Layer-aware processing  </li> </ul>"},{"location":"execution/ingest-engine/#conceptual-execution-flow","title":"Conceptual Execution Flow","text":"<p>The engine follows a predictable and deterministic execution pipeline.</p> <pre><code>\n%%{init: { \n  \"theme\": \"base\",\n  \"flowchart\": { \"nodeSpacing\": 20, \"rankSpacing\": 25, \"curve\": \"basis\" },\n  \"themeVariables\": {\n    \"mainBkg\": \"transparent\",\n    \"lineColor\": \"#9a9a9a\",\n    \"fontSize\": \"14px\"\n  }\n}}%%\n\nflowchart TD\n\n  classDef default fill:transparent,stroke-width:1px,color:#d7d7d7;\n  classDef decision fill:transparent,stroke-width:1px,color:#d7d7d7;\n\n  A[Receive Source DataFrame + Metadata] --&gt; B[Parse Metadata Dictionary]\n  B --&gt; C[Validate Structural Requirements]\n  C --&gt; D[Apply Column Transformations]\n  D --&gt; E[Add Control Columns]\n\n  E --&gt; F{Constraints Enabled?}\n  F -- Yes --&gt; G[Apply Constraint Validation]\n  F -- No --&gt; H[Skip Constraint Layer]\n\n  G --&gt; I[Apply Partition Logic]\n  H --&gt; I\n\n  I --&gt; J[Execute Write Mode]\n  J --&gt; K[Return Structured Execution Result]\n\n  class F decision\n</code></pre> <p>Each step is explicitly derived from metadata definitions.</p>"},{"location":"execution/ingest-engine/#the-ingest-contract","title":"The <code>ingest()</code> Contract","text":"<p>The engine is executed through a single entry point:</p> <p><code>ingest()</code></p> <p>Conceptually, it receives:</p> <ul> <li>a source dataset  </li> <li>a metadata definition  </li> <li>execution configuration  </li> <li>optional runtime parameters  </li> </ul> <p>The metadata determines:</p> <ul> <li>column structure  </li> <li>key definitions  </li> <li>partition strategy  </li> <li>write mode  </li> <li>constraint behavior  </li> <li>operational column handling  </li> </ul> <p>The engine does not infer business logic. All structural decisions must be declared.</p>"},{"location":"execution/ingest-engine/#write-modes","title":"Write Modes","text":"<p>Write behavior is explicitly declared in metadata.</p>"},{"location":"execution/ingest-engine/#append","title":"append","text":"<p>Adds new records without removing existing data.</p>"},{"location":"execution/ingest-engine/#overwrite","title":"overwrite","text":"<p>Replaces target content based on declared strategy.</p>"},{"location":"execution/ingest-engine/#merge-future-ready","title":"merge (future-ready)","text":"<p>Supports key-based upsert logic when primary keys are defined.</p> <p>The engine never infers write behavior.</p>"},{"location":"execution/ingest-engine/#control-and-operational-metadata","title":"Control and Operational Metadata","text":"<p>The engine manages operational traceability through reserved columns such as:</p> <ul> <li><code>_batch_id</code> </li> <li><code>_ingestion_id</code> </li> <li><code>_ingestion_dt</code> </li> <li><code>_partition_&lt;column_name&gt;</code> </li> </ul> <p>These enable:</p> <ul> <li>idempotent execution  </li> <li>incremental strategies  </li> <li>traceability  </li> <li>deterministic reprocessing  </li> </ul> <p>Operational columns are never considered business attributes.</p>"},{"location":"execution/ingest-engine/#idempotency","title":"Idempotency","text":"<p>Ingestia is designed to avoid inconsistent states.</p> <p>Idempotency is achieved through:</p> <ul> <li>deterministic batch identification  </li> <li>explicit write strategies  </li> <li>metadata-controlled partition logic  </li> <li>structured execution boundaries  </li> </ul> <p>Reprocessing the same batch under the same metadata must produce the same result.</p>"},{"location":"execution/ingest-engine/#error-handling-philosophy","title":"Error Handling Philosophy","text":"<p>The engine does not silently ignore structural violations.</p> <p>Execution results are structured and explicit:</p> <ul> <li>status  </li> <li>validation messages  </li> <li>execution metadata  </li> <li>processing metrics  </li> </ul> <p>Failure is visible and traceable.</p> <p>Future extensions may introduce severity levels such as:</p> <ul> <li>ERROR  </li> <li>WARN  </li> <li>QUARANTINE  </li> <li>SKIP  </li> </ul>"},{"location":"execution/ingest-engine/#layer-awareness","title":"Layer Awareness","text":"<p>The engine respects logical layer boundaries:</p> <ul> <li>Raw layer \u2192 minimal structural enforcement  </li> <li>Transformation layer \u2192 standardization and structural rules  </li> <li>Serving layer \u2192 consumption-oriented datasets  </li> </ul> <p>The engine enforces structure but does not dictate modeling methodology.</p>"},{"location":"execution/ingest-engine/#scope-boundaries","title":"Scope Boundaries","text":"<p>The Ingest Engine does not:</p> <ul> <li>enforce surrogate key usage  </li> <li>impose modeling frameworks (Kimball, Inmon, etc.)  </li> <li>manage semantic layer logic  </li> <li>dictate enterprise governance models  </li> </ul> <p>It focuses strictly on deterministic ingestion and structural enforcement.</p>"},{"location":"execution/metadata-model-structure/","title":"Metadata Model Structure","text":"<p>Ingestia is a metadata-driven framework.</p> <p>All execution behavior is derived from structured metadata definitions \u2014 not from hardcoded logic inside pipelines.</p> <p>The Metadata Model defines how structure, constraints, and execution behavior are declared.</p>"},{"location":"execution/metadata-model-structure/#core-principle","title":"Core Principle","text":"<p>Data behavior must be declared, not inferred.</p> <p>The Ingest Engine interprets metadata definitions and translates them into deterministic operations.</p> <p>No structural decision is implicit.</p>"},{"location":"execution/metadata-model-structure/#metadata-architecture","title":"Metadata Architecture","text":"<p>Ingestia uses a simplified and explicit metadata structure:</p> <ul> <li>One metadata dictionary per table</li> <li>One optional integrity dictionary</li> <li>Partition strategy declared at column level</li> </ul> <pre><code>%%{init: { \n  \"theme\": \"base\",\n  \"flowchart\": { \"nodeSpacing\": 26, \"rankSpacing\": 34, \"curve\": \"basis\" },\n  \"themeVariables\": {\n    \"mainBkg\": \"transparent\",\n    \"fontSize\": \"14px\"\n  }\n}}%%\n\nflowchart TB\n  classDef default fill:transparent,stroke-width:1px;\n\n  DD[\"data_dictionary\"] --&gt; TN[\"table_name\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\"]\n\n  TN --&gt; TBL[\"table\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ncatalog\nschema\ndomain\ncontainer\ndescription\npurpose\nwrite_mode\nmerge_schema\ntable_properties\"]\n\n  TN --&gt; COLS[\"columns\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n&lt;source_column_name&gt;\ntarget_column\ndata_type\nnullable\ntransform_expr\nkey\npartition\nz_order\ndelta_column\"]\n\n  TN --&gt; QR[\"quality_rules\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n&lt;rule_name&gt;\ntype\nactive\nseverity\nexpr\"]\n\n  TN --&gt; CS[\"constraints\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n&lt;constraint_name&gt;\ntype\nactive\nseverity\ncolumns\nreference\"]\n\n  CS --&gt; REF[\"reference\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ncatalog\nschema\ntable\ncolumns\"]</code></pre>"},{"location":"execution/metadata-model-structure/#table-metadata-dictionary","title":"Table Metadata Dictionary","text":"<p>Each table has its own metadata dictionary.</p> <p>It defines:</p> <ul> <li>target layer</li> <li>domain</li> <li>object name</li> <li>column structure</li> <li>write behavior</li> <li>control column activation</li> <li>integrity reference (optional)</li> </ul> <p>This dictionary is the single source of truth for table behavior.</p>"},{"location":"execution/metadata-model-structure/#column-level-partition-strategy","title":"Column-Level Partition Strategy","text":"<p>Partitioning is declared directly within column definitions.</p> <p>There is no separate partition dictionary.</p> <p>If a column participates in partitioning:</p> <ul> <li>it is declared explicitly</li> <li>helper columns such as <code>_partition_&lt;column_name&gt;</code> may be generated</li> <li>derivation logic is deterministic</li> </ul> <p>Partition behavior is structural, not inferred.</p>"},{"location":"execution/metadata-model-structure/#integrity-dictionary","title":"Integrity Dictionary","text":"<p>The integrity dictionary is a separate structure referenced by the table metadata.</p> <p>It may define:</p> <ul> <li>primary key rules</li> <li>unique combinations</li> <li>referential integrity</li> <li>structural validations</li> </ul> <p>This separation ensures:</p> <ul> <li>structural metadata remains clean</li> <li>constraint logic remains reusable</li> <li>integrity rules can evolve independently</li> </ul> <p>The integrity dictionary is optional but recommended for governed environments.</p>"},{"location":"execution/metadata-model-structure/#execution-relationship","title":"Execution Relationship","text":"Responsibility Table Metadata Integrity Dictionary Engine Structure Yes No Executes Keys Yes May validate Executes Constraints No Yes Executes Partitioning Yes (column-level) No Executes Write Mode Yes No Executes <p>The engine interprets both dictionaries during execution.</p>"},{"location":"execution/metadata-model-structure/#deterministic-design","title":"Deterministic Design","text":"<p>Because metadata is explicit:</p> <ul> <li>execution is reproducible</li> <li>structural behavior is auditable</li> <li>engine logic remains stable</li> <li>changes are configuration-driven</li> </ul> <p>The engine evolves independently from metadata definitions.</p>"},{"location":"execution/metadata-model-structure/#scope-boundaries","title":"Scope Boundaries","text":"<p>The Metadata Model does not:</p> <ul> <li>enforce a specific modeling methodology</li> <li>impose naming conventions beyond reserved patterns</li> <li>dictate enterprise governance models</li> </ul> <p>It provides a declarative structural control plane for the Lakehouse.</p>"},{"location":"foundations/architecture/","title":"Architecture","text":""},{"location":"foundations/architecture/#architecture-is-contextual","title":"Architecture Is Contextual","text":"<p>There is no universal architecture.</p> <p>A good architecture is not the most sophisticated one. It is the one that is consistently applied, understood, and maintained.</p> <p>Every client operates within a specific context:</p> <ul> <li>organizational structure</li> <li>data maturity</li> <li>regulatory requirements</li> <li>platform constraints</li> <li>team skillset</li> </ul> <p>Ingestia does not prescribe a rigid architectural model. It provides a reference approach that has proven effective in real-world environments and remains adaptable to context.</p> <p>Consistency matters more than perfection.</p>"},{"location":"foundations/architecture/#the-problem-modern-architectures-face","title":"The Problem Modern Architectures Face","text":"<p>Modern data platforms must deal with:</p> <ul> <li>multiple providers and ingestion patterns  </li> <li>cross-domain transformations  </li> <li>evolving schemas  </li> <li>increasing governance expectations  </li> <li>distributed teams working in parallel  </li> </ul> <p>Without architectural discipline, ingestion becomes inconsistent. Without operational execution, governance becomes theoretical.</p> <p>Ingestia addresses this gap by aligning architecture, metadata, and execution into a single operational model.</p>"},{"location":"foundations/architecture/#reference-architectural-model","title":"Reference Architectural Model","text":"<p>The reference implementation of Ingestia follows a Lakehouse-based approach composed of structured layers:</p> <ul> <li>Raw \u2014 data stored as received</li> <li>Standardized \u2014 typed, cleaned, structurally aligned</li> <li>Conformed \u2014 cross-source canonical models</li> <li>Serving \u2014 optimized for analytical consumption</li> </ul> <p>This layering strategy is not a theoretical exercise. It defines responsibilities, boundaries, and transformation rules.</p> <p>Ingestia also distinguishes between:</p> <ul> <li>Platform-level components (shared execution logic, metadata handling, enforcement mechanisms)</li> <li>Domain-level models (sales, marketing, finance, etc.)</li> </ul> <p>This separation enables scalability without sacrificing governance.</p> <p>The detailed rules for each layer are described in the Layering Strategy section.</p>"},{"location":"foundations/architecture/#methodology-first-technology-second","title":"Methodology First, Technology Second","text":"<p>Ingestia is a methodology first and a set of libraries second.</p> <p>To operationalize the methodology, a concrete implementation was necessary. The initial implementation was developed using:</p> <ul> <li>PySpark</li> <li>Databricks Lakehouse</li> <li>Unity Catalog</li> <li>Azure Data Lake Storage (ADLS)</li> <li>Azure Data Factory (ADF)</li> </ul> <p>These technologies were chosen because they align with the framework\u2019s principles:</p> <ul> <li>distributed and deterministic processing</li> <li>clear separation of storage and compute</li> <li>scalable metadata governance</li> <li>structured orchestration</li> </ul> <p>However, the architectural principles described in this documentation are not bound to these tools.</p> <p>The metadata-driven approach, layering strategy, and integrity enforcement model can be implemented in other ecosystems.</p> <p>The current technology stack represents a pragmatic starting point \u2014 not a limitation.</p>"},{"location":"foundations/architecture/#starting-somewhere","title":"Starting Somewhere","text":"<p>Architecture must eventually leave theory and enter execution.</p> <p>Ingestia was implemented in Databricks because it provided the closest alignment with the desired operational model at the time of development.</p> <p>A methodology only becomes real when it is tested under production pressure.</p> <p>The framework evolved through practical application \u2014 across real domains, real providers, and real governance constraints.</p> <p>As technology evolves, the implementation may evolve.</p> <p>The philosophy and architectural discipline remain.</p>"},{"location":"foundations/naming-conventions/","title":"Naming Conventions","text":"<p>Ingestia does not enforce a single naming convention for your entire data platform. However, a consistent naming strategy is required to ensure clarity, automation, and reliability.</p> <p>The framework reserves only a small set of mandatory patterns required for its internal operation. Outside of those reserved patterns, organizations are free to adopt their own standards \u2014 as long as they remain consistent.</p>"},{"location":"foundations/naming-conventions/#physical-and-logical-naming","title":"Physical and Logical Naming","text":"<p>In Ingestia, physical and logical names are identical.</p> <p>There is no abstraction or aliasing layer between catalog objects and pipeline references. The name defined in the catalog is the name used by the engine.</p>"},{"location":"foundations/naming-conventions/#layer-structure","title":"Layer Structure","text":"<p>The conceptual responsibilities of each layer are described in Layering Strategy. Below is how naming may reflect that structure.</p> Logical Layer Suggested Schema / Prefix Pattern Raw <code>raw_&lt;domain&gt;.&lt;object&gt;</code> Transformation <code>transformation_&lt;domain&gt;.&lt;object&gt;</code> \u2514\u2013 Conformed <code>transformation_&lt;domain&gt;.&lt;object&gt;_conformed</code> Serving <code>serving_&lt;subject&gt;.&lt;object&gt;</code> <p>Ingestia does not enforce these exact prefixes, but consistency within the environment is required.</p>"},{"location":"foundations/naming-conventions/#framework-reserved-columns","title":"Framework-Reserved Columns","text":"<p>Framework-controlled columns must begin with an underscore.</p> <p>Typical examples:</p> <p><code>_batch_id</code> <code>_ingestion_id</code> <code>_ingestion_dt</code> <code>_source_file</code> <code>_partition_&lt;column_name&gt;</code></p> <p>Mandatory rules:</p> <ul> <li>lowercase only  </li> <li>snake_case  </li> <li>explicit and readable naming  </li> </ul> <p>Columns starting with an underscore are considered operational or technical. Columns without underscore are considered business attributes.</p>"},{"location":"foundations/naming-conventions/#partitioning-rules","title":"Partitioning Rules","text":"<p>When a technical partition column is required, it must follow this exact pattern:</p> <p><code>_partition_&lt;column_name&gt;</code></p> <p>Example:</p> <p><code>_partition_year_month</code></p> <p>Rules:</p> <ul> <li>snake_case  </li> <li>lowercase  </li> <li>written in full words  </li> <li>avoid ambiguous abbreviations (for example, avoid _partition_yyyymm)  </li> </ul> <p>Partition columns are always considered operational metadata, not business attributes.</p>"},{"location":"foundations/naming-conventions/#key-suffixes-optional","title":"Key Suffixes (Optional)","text":"<p>If your organization uses suffixes to differentiate key types, common patterns include:</p> <p><code>_cd</code> \u2192 natural key / business code <code>_id</code> \u2192 surrogate key / technical identifier  </p> <p>These suffixes are not required by Ingestia. Other naming patterns may be used, provided they remain consistent across the environment.</p>"},{"location":"foundations/naming-conventions/#pk-and-fk-naming-consistency","title":"PK and FK Naming Consistency","text":"<p>A recommended best practice:</p> <p>The name of the primary key (PK) in the dimension should be exactly the same as the foreign key (FK) in the fact table.</p> <p>This improves readability, simplifies joins, and reduces ambiguity across the model.</p> <p>Ingestia strongly recommends this practice, but does not enforce it.</p>"},{"location":"foundations/naming-conventions/#example-pk-fk","title":"Example (PK = FK)","text":"<pre><code>%%{init: { \"theme\": \"base\", \"themeVariables\": {\n  \"fontFamily\": \"Inter, system-ui, -apple-system, Segoe UI, Roboto, Arial\",\n  \"fontSize\": \"14px\"\n}}}%%\n\nerDiagram\n  product {\n    int product_id PK\n    string ean_cd\n    string product_nm\n    string brand_nm\n    string category_nm\n  }\n\n  store {\n    int store_id PK\n    string store_cd\n    string store_nm\n    string city_nm\n    string state_cd\n  }\n\n  customer {\n    int customer_id PK\n    string customer_nm\n    string segment_nm\n  }\n\n  period {\n    int period_id PK\n    int year_nr\n    int month_nr\n    date period_start_dt\n  }\n\n  sellout {\n    bigint sellout_id PK\n    int period_id FK\n    int product_id FK\n    int store_id FK\n    int customer_id FK\n    float sellout_qt\n    float sellout_vl\n    string data_provider_cd\n    datetime ingestion_ts\n  }\n\n  product  ||--o{ sellout : has\n  store    ||--o{ sellout : has\n  customer ||--o{ sellout : has\n  period   ||--o{ sellout : has\n</code></pre>"},{"location":"foundations/naming-conventions/#delta-table-naming-requirements","title":"Delta Table Naming Requirements","text":"<p>Tables and columns must comply with valid Delta Lake and Databricks naming rules.</p> <p>In general:</p> <ul> <li>no spaces  </li> <li>avoid special characters (except underscore)  </li> <li>do not start names with numbers  </li> <li>avoid SQL reserved keywords  </li> </ul> <p>For the official and most up-to-date rules regarding valid identifiers, refer to the Databricks SQL documentation:</p> <p>Databricks SQL Language Manual \u2013 Object Names https://learn.microsoft.com/azure/databricks/sql/language-manual/sql-ref-names</p> <p>Ensuring compliance with these rules guarantees compatibility with Delta Lake operations, catalog management, and automated execution within Ingestia.</p>"},{"location":"foundations/naming-conventions/#ingestia-naming-convention-template","title":"Ingestia Naming Convention Template","text":"<p>Section under development</p> <p>A formal version of the Ingestia Style naming convention may be published in the future.</p>"},{"location":"foundations/philosophy/","title":"Philosophy","text":""},{"location":"foundations/philosophy/#listening-to-the-business-20-years-later","title":"Listening to the Business \u2014 20 Years Later","text":"<p>In The Data Warehouse Toolkit, Ralph Kimball closes the book with a reminder that remains timeless:</p> <p>\u201cThe gold coin for the data warehouse professional is to listen to the business. After all, our job is to publish the right data.\u201d</p> <p>More than twenty years have passed.</p> <p>Technology has changed. Architectures have evolved. Lakehouses replaced traditional warehouses. Cloud platforms accelerated everything. Organizational complexity increased.</p> <p>But the objective did not change.</p> <p>Our job is still to publish the right data.</p>"},{"location":"foundations/philosophy/#the-modern-tension","title":"The Modern Tension","text":"<p>Today, data teams operate under constant pressure:</p> <ul> <li>faster delivery cycles  </li> <li>multiple data providers  </li> <li>cross-domain transformations  </li> <li>high expectations for traceability and governance  </li> </ul> <p>In this environment, documentation and architectural standards often become one of two extremes:</p> <ol> <li>Over-engineered and disconnected from execution, slowing delivery.</li> <li>Ignored in favor of speed, creating opaque and ungovernable environments.</li> </ol> <p>Both paths lead away from publishing the right data.</p> <p>Speed without structure creates chaos. Structure without execution creates bureaucracy.</p>"},{"location":"foundations/philosophy/#documentation-as-a-means-not-an-end","title":"Documentation as a Means, Not an End","text":"<p>Ingestia was born from a simple conviction:</p> <p>Documentation should not be the final artifact of a project. It should be a parameter of execution.</p> <p>Architectural standards, naming conventions, integrity rules, and lifecycle definitions must not live in:</p> <ul> <li>static PDFs</li> <li>isolated wikis</li> <li>individual developer knowledge</li> <li>hidden notebook logic</li> </ul> <p>They must be executable.</p> <p>When documentation becomes metadata, and metadata drives ingestion, governance stops being a bottleneck and becomes an enabler.</p>"},{"location":"foundations/philosophy/#transparency-over-tribal-knowledge","title":"Transparency Over Tribal Knowledge","text":"<p>A data platform cannot depend on implicit knowledge.</p> <p>The lifecycle of data must be explicit to the organization:</p> <ul> <li>where data originated</li> <li>how it was standardized</li> <li>how it was transformed</li> <li>which rules were enforced</li> <li>which constraints were validated</li> <li>what was rejected and why</li> </ul> <p>This visibility cannot rely on the memory of a developer or the informal understanding of a key user.</p> <p>It must be encoded, enforced, and reproducible.</p>"},{"location":"foundations/philosophy/#evolving-the-discipline","title":"Evolving the Discipline","text":"<p>Ingestia does not reject data warehousing discipline. It evolves it.</p> <p>The foundational idea remains the same: publish the right data.</p> <p>What changes is how we operationalize that idea in a Lakehouse world:</p> <ul> <li>through metadata-driven ingestion</li> <li>through explicit layering strategies</li> <li>through executable integrity rules</li> <li>through standardized control columns</li> <li>through deterministic execution patterns</li> </ul> <p>The philosophy is not dogmatic. It is pragmatic and continuously evolving.</p> <p>Technology will keep changing. Organizational pressures will keep increasing.</p> <p>The horizon remains the same.</p> <p>Our responsibility is still to publish the right data \u2014 but now with transparency, reproducibility, and governance built into execution itself.</p>"},{"location":"guidelines/driagrams/","title":"Mermaid Examples","text":"<pre><code>%%{init: {\n  \"flowchart\": {\n    \"nodeSpacing\": 20,\n    \"rankSpacing\": 25\n  }\n}}%%\nflowchart LR\n  classDef card fill:transparent,stroke-width:1px;\n\n  RAW[Raw Layer]:::card --&gt; STD[Standardized Layer]:::card\n  STD --&gt; CNF[Conformed Layer]:::card\n  CNF --&gt; SRV[Serving Layer]:::card\n\n</code></pre> <pre><code>%%{init: { \"theme\": \"base\", \"themeVariables\": {\n  \"fontFamily\": \"Inter, system-ui, -apple-system, Segoe UI, Roboto, Arial\",\n  \"fontSize\": \"14px\"\n}}}%%\n\nerDiagram\n  DIM_PRODUCT {\n    int product_id PK\n    string ean_cd\n    string product_nm\n    string brand_nm\n    string category_nm\n  }\n\n  DIM_STORE {\n    int store_id PK\n    string store_cd\n    string store_nm\n    string city_nm\n    string state_cd\n  }\n\n  DIM_CUSTOMER {\n    int customer_id PK\n    string sold_to_party_cd\n    string customer_nm\n    string segment_nm\n  }\n\n  DIM_PERIOD {\n    int period_id PK\n    int year_nr\n    int month_nr\n    date period_start_dt\n  }\n\n  FACT_SELLOUT {\n    bigint sellout_id PK\n    int period_id FK\n    int product_id FK\n    int store_id FK\n    int customer_id FK\n    float sellout_qt\n    float sellout_vl\n    string data_provider_cd\n    datetime ingestion_ts\n  }\n\n  DIM_PRODUCT  ||--o{ FACT_SELLOUT : has\n  DIM_STORE    ||--o{ FACT_SELLOUT : has\n  DIM_CUSTOMER ||--o{ FACT_SELLOUT : has\n  DIM_PERIOD   ||--o{ FACT_SELLOUT : has\n</code></pre>"},{"location":"guidelines/patterns/","title":"Patterns","text":"<p>Section under development</p> <p>This module is fully designed. Detailed documentation is being progressively published.</p>"},{"location":"model/layering/","title":"Layering Strategy","text":""},{"location":"model/layering/#the-medallion-concept-and-its-adaptation","title":"The Medallion Concept \u2014 and Its Adaptation","text":"<p>The Medallion Architecture (Bronze, Silver, Gold) was popularized in the Lakehouse ecosystem as a structured way to organize data refinement.</p> <p>Ingestia adopts this concept but applies it with explicit operational boundaries and governance rules.</p> <p>Instead of blindly replicating the pattern, the framework defines clear responsibilities for each layer \u2014 both physical and logical.</p> <p>Ingestia operates with three physical layers:</p> <ul> <li>Raw (Bronze)</li> <li>Transformation (Silver)</li> <li>Serving (Gold)</li> </ul> <p>Within the Transformation layer, two logical stages are defined:</p> <ul> <li>Standardized</li> <li>Conformed</li> </ul> <p>This distinction is essential for maintaining clarity and scalability.</p>"},{"location":"model/layering/#raw-bronze","title":"Raw (Bronze)","text":"<p>Raw is not a transformation layer.</p> <p>It is a storage layer.</p> <p>In this layer:</p> <ul> <li>Data is stored exactly as received.</li> <li>No business transformation is applied.</li> <li>No Spark-managed tables are created.</li> <li>Ingestion writes directly to the lake storage.</li> <li>Partitioning is controlled by ingestion metadata.</li> </ul> <p>Key principles:</p> <ul> <li>Raw data is never modified.</li> <li>Raw data is never deduplicated.</li> <li>Raw data is never merged.</li> <li>Raw preserves ingestion history.</li> </ul> <p>Raw exists to guarantee traceability and reproducibility.</p> <p>It is the source of truth for ingestion \u2014 not for analytics.</p>"},{"location":"model/layering/#transformation-silver","title":"Transformation (Silver)","text":"<p>From this layer forward, data is treated as structured tables.</p> <p>Transformation is divided logically into two stages:</p>"},{"location":"model/layering/#standardized","title":"Standardized","text":"<p>The Standardized stage focuses on structural alignment.</p> <p>Here we:</p> <ul> <li>Apply type casting.</li> <li>Normalize column names.</li> <li>Enforce naming conventions.</li> <li>Introduce control columns.</li> <li>Apply structural integrity validations.</li> <li>Preserve provider-specific grain.</li> </ul> <p>Important:</p> <ul> <li>No cross-provider merging occurs here.</li> <li>No aggregation is performed.</li> <li>The objective is structural consistency, not canonical modeling.</li> </ul> <p>Standardized prepares data for integration without altering its semantic origin.</p>"},{"location":"model/layering/#conformed","title":"Conformed","text":"<p>The Conformed stage unifies data across sources and domains.</p> <p>Here we:</p> <ul> <li>Align canonical keys.</li> <li>Merge multiple providers.</li> <li>Harmonize domain logic.</li> <li>Define cross-source models.</li> <li>Maintain controlled duplication when necessary.</li> </ul> <p>Data may be reorganized, but its transformation remains governed and traceable.</p> <p>Conformed models represent enterprise-aligned structures \u2014 not consumption-optimized artifacts.</p>"},{"location":"model/layering/#serving-gold","title":"Serving (Gold)","text":"<p>Serving is the consumption layer.</p> <p>Here, the focus shifts from structural integrity to purpose optimization.</p> <p>In this layer:</p> <ul> <li>Redundancy is allowed and intentional.</li> <li>Data may be aggregated.</li> <li>Models may be reshaped for BI, reporting, or data science.</li> <li>Performance and access patterns are prioritized.</li> </ul> <p>Examples include:</p> <ul> <li>Power BI data marts</li> <li>Feature tables for machine learning</li> <li>Domain-specific analytical views</li> </ul> <p>All redundancy in Serving must be governed by the controlled origin defined in Transformation.</p> <p>Serving is optimized. Transformation is disciplined.</p>"},{"location":"model/layering/#physical-vs-logical-separation","title":"Physical vs Logical Separation","text":"<p>Ingestia separates layers physically:</p> <ul> <li>Raw, Transformation, and Serving are isolated at storage level.</li> </ul> <p>Within Transformation, Standardized and Conformed are logically separated to preserve clarity of responsibility.</p> <p>This approach:</p> <ul> <li>Prevents accidental cross-layer leakage.</li> <li>Improves governance.</li> <li>Simplifies troubleshooting.</li> <li>Supports scalable domain growth.</li> </ul>"},{"location":"model/layering/#why-this-matters","title":"Why This Matters","text":"<p>Without clear layering boundaries:</p> <ul> <li>Raw becomes polluted.</li> <li>Standardized becomes business-driven.</li> <li>Conformed becomes uncontrolled.</li> <li>Serving becomes a shadow warehouse.</li> </ul> <p>Layering is not about aesthetics.</p> <p>It is about enforceable discipline.</p>"},{"location":"model/layering/#references","title":"References","text":"<p>The Medallion Architecture was popularized in the Lakehouse paradigm, particularly within the Databricks ecosystem.</p> <p>Ingestia adopts the layered refinement principle while introducing explicit governance, metadata-driven execution, and operational enforcement across layers.</p>"},{"location":"model/metadata-driven-architecture/","title":"Metadata-Driven Architecture","text":""},{"location":"model/metadata-driven-architecture/#documentation-as-execution-input","title":"Documentation as Execution Input","text":"<p>Ingestia is built on a simple but transformative idea:</p> <p>Metadata does not describe the system. Metadata drives the system.</p> <p>Instead of spreading structural definitions across notebooks, pipelines, and implicit conventions, Ingestia centralizes execution parameters inside explicit dictionaries.</p> <p>These dictionaries are not passive documentation artifacts.</p> <p>They are consumed directly by the ingestion engine.</p>"},{"location":"model/metadata-driven-architecture/#catalog-schema-and-layering","title":"Catalog, Schema, and Layering","text":"<p>Ingestia assumes a single catalog at the platform level.</p> <p>Within that catalog, schemas are defined by combining:</p> <pre><code>&lt;layer&gt;_&lt;domain&gt;\n</code></pre> <p>Examples:</p> <ul> <li><code>raw_sales</code></li> <li><code>transformation_sales</code></li> <li><code>serving_sales</code></li> </ul> <p>The layer comes first because lifecycle responsibility precedes business domain.</p> <p>This structure enforces architectural clarity at the metadata level.</p>"},{"location":"model/metadata-driven-architecture/#table-centric-dictionary","title":"Table-Centric Dictionary","text":"<p>Ingestia does not define generic models.</p> <p>Each table has its own explicit dictionary.</p> <p>The dictionary defines how this table behaves within its layer and domain.</p>"},{"location":"model/metadata-driven-architecture/#example-sellout-data-dictionary","title":"Example: Sellout Data Dictionary","text":"<p>Below is a simplified example of a dictionary used for a table in the schema.</p> <pre><code>{\n  \"data_dictionary\": {\n    \"&lt;table_name&gt;\": {\n      \"table\": {\n        \"catalog\": \"main\",\n        \"schema\": \"&lt;layer&gt;_&lt;domain&gt;\",\n        \"domain\": \"&lt;domain&gt;\",\n        \"container\": \"raw | transformation | serving\",\n        \"description\": \"&lt;table description&gt;\",\n        \"purpose\": \"raw | standardized | conformed | serving\",\n\n        \"write_mode\": \"append | overwrite | merge\",\n        \"merge_schema\": true,\n\n        \"table_properties\": {\n          \"delta.autoOptimize.optimizeWrite\": \"true\",\n          \"delta.enableChangeDataFeed\": \"true\"\n        }\n      },\n\n      \"columns\": {\n        \"&lt;source_column_name&gt;\": {\n          \"target_column\": \"&lt;target_column_name&gt;\",\n          \"data_type\": \"string | int | bigint | decimal(18,2) | date | timestamp | boolean\",\n          \"nullable\": true,\n          \"transform_expr\": \"&lt;spark_sql_expression_optional&gt;\",\n          \"key\": false,\n          \"partition\": false,\n          \"z_order\": false,\n          \"delta_column\": false\n        }\n      },\n\n      \"quality_rules\": {\n        \"&lt;rule_name&gt;\": {\n          \"type\": \"expression | not_null | in_list | regex | range | custom\",\n          \"active\": true,\n          \"severity\": \"ERROR | WARN | QUARANTINE | SKIP\",\n          \"expr\": \"&lt;spark_sql_boolean_expression_optional&gt;\",\n          \"params\": {}\n        }\n      },\n\n      \"constraints\": {\n        \"&lt;constraint_name&gt;\": {\n          \"type\": \"primary_key | foreign_key | unique | check\",\n          \"active\": true,\n          \"severity\": \"ERROR | WARN | QUARANTINE | SKIP\",\n          \"columns\": [\"&lt;col1&gt;\", \"&lt;col2&gt;\"],\n\n          \"reference\": {\n            \"catalog\": \"main\",\n            \"schema\": \"&lt;ref_schema&gt;\",\n            \"table\": \"&lt;ref_table&gt;\",\n            \"columns\": [\"&lt;ref_col1&gt;\"]\n          },\n\n          \"check_expr\": \"&lt;spark_sql_boolean_expression_optional&gt;\"\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"model/metadata-driven-architecture/#separation-of-concerns","title":"Separation of Concerns","text":"<p>The metadata model separates concerns inside a single, structured dictionary:</p> <ul> <li>Structural definition (<code>columns</code>, types, nullability)</li> <li>Execution behavior (<code>write_mode</code>, partitioning, table properties)</li> <li>Data quality validation (<code>quality_rules</code>)</li> <li>Integrity enforcement (<code>constraints</code>)</li> <li>Lifecycle responsibility (<code>container</code>)</li> <li>Business alignment (<code>domain</code>)</li> </ul> <p>Each concern is explicit, declarative, and versionable.</p> <p>No execution logic is hard-coded.</p> <p>The ingestion engine consumes this dictionary and applies behavior deterministically.</p>"},{"location":"model/metadata-driven-architecture/#why-this-matters","title":"Why This Matters","text":"<p>Without a metadata-driven approach:</p> <ul> <li>Schema evolution becomes inconsistent.</li> <li>Naming conventions drift.</li> <li>Integrity rules are optional.</li> <li>Reproducibility depends on individual developers.</li> <li>Governance lives outside execution.</li> </ul> <p>With a metadata-driven model:</p> <ul> <li>Standards are enforced.</li> <li>Behavior is deterministic.</li> <li>Architecture becomes executable.</li> <li>Delivery speed increases without sacrificing control.</li> </ul> <p>Metadata becomes the contract between architecture and execution.</p> <p>That contract is what makes Ingestia scalable.</p>"},{"location":"project/governance/","title":"Governance","text":""},{"location":"project/governance/#maintainer","title":"Maintainer","text":"<p>Adler Teodoro Creator and Lead Architect of Ingestia  </p> <p>Contact:</p> <ul> <li>LinkedIn: Adler Teodoro</li> <li>Email: teodoro@adler.technology</li> </ul> <p>The maintainer is responsible for the architectural direction, standards, and evolution of the Ingestia framework.</p>"},{"location":"project/governance/#contributors","title":"Contributors","text":"<p>The following professionals have contributed to the development and evolution of Ingestia:</p> <p>(List to be updated.)</p>"},{"location":"project/governance/#how-to-contribute","title":"How to Contribute","text":"<p>Ingestia welcomes contributions in areas such as architecture, implementation, documentation, and performance improvements.</p> <p>To contribute, please contact the maintainer directly.</p>"},{"location":"project/overview/","title":"Ingestia","text":"<p>Ingestia is a metadata-driven framework designed to transform documentation, architectural standards, and governance from static artifacts into executable parameters.</p> <p>Instead of treating documentation as the final step of a project, Ingestia makes it the starting point of execution.</p> <p>Architecture rules, naming standards, and integrity definitions are not described \u2014 they are enforced.</p>"},{"location":"project/overview/#why-ingestia-exists","title":"Why Ingestia Exists","text":"<p>Modern Lakehouse projects operate under constant pressure:</p> <ul> <li>faster delivery cycles</li> <li>increasing data sources</li> <li>multiple domains working in parallel</li> <li>high expectations for data quality and traceability</li> </ul> <p>In this environment, documentation and governance often become bottlenecks.</p> <p>When over-engineered, they slow delivery. When ignored, they create chaos.</p> <p>Ingestia was created to resolve this tension.</p> <p>It ensures that:</p> <ul> <li>standards do not slow down execution</li> <li>governance is not optional</li> <li>ingestion is consistent across teams</li> <li>quality rules are executable, not theoretical</li> </ul>"},{"location":"project/overview/#the-core-principle","title":"The Core Principle","text":"<p>Documentation is not an artifact. It is a parameter.</p> <p>Ingestia is built around a metadata-driven approach where:</p> <ul> <li>structural definitions</li> <li>naming conventions</li> <li>integrity rules</li> <li>execution behavior</li> </ul> <p>are defined in dictionaries and consumed directly by the ingestion engine.</p> <p>This eliminates hidden logic inside notebooks and pipelines.</p> <p>Execution becomes deterministic, reproducible, and transparent.</p>"},{"location":"project/overview/#what-ingestia-provides","title":"What Ingestia Provides","text":"<p>Ingestia defines a structured Lakehouse model composed of:</p> <ul> <li>Layering strategy (Raw, Standardized, Conformed, Serving)</li> <li>Metadata dictionaries that describe structure and constraints</li> <li>Execution engine that applies those definitions consistently</li> <li>Control columns and batching standards</li> <li>Integrity enforcement and quarantine strategy</li> </ul> <p>The result is a system where:</p> <ul> <li>projects remain standardized</li> <li>delivery speed is maintained</li> <li>governance scales with growth</li> <li>environments remain manageable over time</li> </ul>"},{"location":"project/overview/#who-it-is-for","title":"Who It Is For","text":"<p>Ingestia is designed for:</p> <ul> <li>Data Architects defining cross-domain standards</li> <li>Data Engineers building ingestion and transformation pipelines</li> <li>Platform teams implementing scalable LakeOps practices</li> </ul> <p>It is not a no-code ingestion tool. It is not a data warehouse methodology.</p> <p>It is an execution framework for disciplined Lakehouse architecture.</p>"},{"location":"project/overview/#what-you-will-find-in-this-documentation","title":"What You Will Find in This Documentation","text":"<p>This documentation describes:</p> <ul> <li>The philosophy and design principles behind Ingestia</li> <li>The architectural model and layering strategy</li> <li>The metadata-driven structure that powers execution</li> <li>The ingestion engine and enforcement mechanisms</li> <li>The governance and quality model</li> <li>Patterns and anti-patterns observed in real-world projects</li> </ul> <p>If your goal is to deliver fast without sacrificing standards, Ingestia was built for that purpose.</p>"}]}